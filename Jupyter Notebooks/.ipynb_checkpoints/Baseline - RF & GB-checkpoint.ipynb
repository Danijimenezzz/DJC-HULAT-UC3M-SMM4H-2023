{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f51a7f",
   "metadata": {},
   "source": [
    "## Multi-class classification of sentiment associated with therapies in English tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be53c2",
   "metadata": {},
   "source": [
    "### Read in & clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5276bf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>therapy</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1550591923047600131</td>\n",
       "      <td>cannabis</td>\n",
       "      <td>@chuckschumer YES. Please. Cannabis is legal i...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1496301299691839491</td>\n",
       "      <td>adderall</td>\n",
       "      <td>@youdoingtoomuch I’m a busy girl, adderall kee...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1460587790966657024</td>\n",
       "      <td>adderall</td>\n",
       "      <td>adderall adderall caffeine caffeine caffeine k...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1393586192625528832</td>\n",
       "      <td>alprazolam</td>\n",
       "      <td>@justky1018 See if you can get your doctor to ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1561452418285547520</td>\n",
       "      <td>diazepam</td>\n",
       "      <td>@feytaline Reminds me of the time I had a roug...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id     therapy  \\\n",
       "0  1550591923047600131    cannabis   \n",
       "1  1496301299691839491    adderall   \n",
       "2  1460587790966657024    adderall   \n",
       "3  1393586192625528832  alprazolam   \n",
       "4  1561452418285547520    diazepam   \n",
       "\n",
       "                                                text     label  \n",
       "0  @chuckschumer YES. Please. Cannabis is legal i...   neutral  \n",
       "1  @youdoingtoomuch I’m a busy girl, adderall kee...  positive  \n",
       "2  adderall adderall caffeine caffeine caffeine k...   neutral  \n",
       "3  @justky1018 See if you can get your doctor to ...   neutral  \n",
       "4  @feytaline Reminds me of the time I had a roug...  positive  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\train.csv\")\n",
    "dev_data = pd.read_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\dev.csv\")\n",
    "\n",
    "# Concatenate the train and dev data\n",
    "data = pd.concat([train_data, dev_data])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617c4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "# Downloading the stopwords corpus from NLTK (words like \"the\", \"is\", \"and\" that are \n",
    "# commonly used and can be ignored)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Creating a Porter stemmer object from NLTK (used for stemming words to their base form)\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "# Reading the data from the \"train.csv\" file into a Pandas DataFrame\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\train.csv\")\n",
    "\n",
    "# Function to count the percentage of punctuation characters in a given text\n",
    "def count_punct(text):\n",
    "    # Counting the number of punctuation characters in the text\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    # Calculating the percentage of punctuation characters (excluding spaces) in the text\n",
    "    return round(count/(len(text) - text.count(\" \")), 3) * 100\n",
    "\n",
    "# Applying the 'count_punct' function to the 'text' column and storing the result in \n",
    "# a new 'body_len' column\n",
    "data['body_len'] = data['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# Applying the 'count_punct' function to the 'body_text' column and storing the result in \n",
    "# a new 'punct%' column\n",
    "data['punct%'] = data['text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# Function to clean the text by removing punctuation, converting to lowercase, and stemming words\n",
    "def clean_text(text):\n",
    "    # Removing punctuation characters from the text and converting it to lowercase\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    # Splitting the text into tokens (words) using regular expressions\n",
    "    tokens = re.split('\\W+', text)\n",
    "    # Stemming each word in the tokens list using the Porter stemmer\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    # Returning the cleaned text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82ea1f",
   "metadata": {},
   "source": [
    "### Add word sentiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0619998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\danij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ad144",
   "metadata": {},
   "source": [
    "Define una función que calcule el sentimiento de cada palabra en un texto, utilizando el léxico de sentimientos apropiado. Aquí tienes un ejemplo utilizando SentiWordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3932903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        sentiment = synsets[0].pos_score() - synsets[0].neg_score()\n",
    "        return sentiment\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cf6f1",
   "metadata": {},
   "source": [
    "Itera sobre cada texto en tu dataset y para cada palabra en el texto, utiliza la función get_word_sentiment para obtener el sentimiento de esa palabra. Puedes almacenar los sentimientos en una nueva lista o como una columna adicional en tu dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca021e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SOFTWARE\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data['sentiments'] = data['text'].apply(lambda text: np.mean([get_word_sentiment(word) for word in nltk.word_tokenize(text) if get_word_sentiment(word) != 0]))\n",
    "\n",
    "data['sentiments'] = data['sentiments'].fillna(np.mean(data['sentiments']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24604c07",
   "metadata": {},
   "source": [
    "### Add more sentiment related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2af3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import treebank\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the opinion lexicon\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "# This tokenizer uses the Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank\n",
    "tokenizer = treebank.TreebankWordTokenizer()\n",
    "\n",
    "# Create a Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to count positive and negative words\n",
    "def count_pos_neg_words(text):\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "\n",
    "    tokenized_text = [word.lower() for word in tokenizer.tokenize(text)]\n",
    "\n",
    "    for word in tokenized_text:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "\n",
    "    return pos_words, neg_words\n",
    "\n",
    "# Apply the function to the text column\n",
    "data['pos_word_count'], data['neg_word_count'] = zip(*data['text'].map(count_pos_neg_words))\n",
    "\n",
    "# Function to get sentiment intensity\n",
    "def get_sentiment_intensity(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "# Apply the function to the text column\n",
    "data['sentiment_intensity'] = data['text'].apply(get_sentiment_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb9477",
   "metadata": {},
   "source": [
    "### Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd84c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train_test_split function from the sklearn.model_selection module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "# The 'body_text', 'body_len', and 'punct%' columns are used as the features (X)\n",
    "# The 'label' column is used as the target variable (y)\n",
    "# The test_size parameter is set to 0.2, which means 20% of the data will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['text', 'body_len', 'punct%', 'sentiments']], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f9d915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>therapy</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1550591923047600131</td>\n",
       "      <td>cannabis</td>\n",
       "      <td>@chuckschumer YES. Please. Cannabis is legal i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>239</td>\n",
       "      <td>6.3</td>\n",
       "      <td>-0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1496301299691839491</td>\n",
       "      <td>adderall</td>\n",
       "      <td>@youdoingtoomuch I’m a busy girl, adderall kee...</td>\n",
       "      <td>positive</td>\n",
       "      <td>67</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1460587790966657024</td>\n",
       "      <td>adderall</td>\n",
       "      <td>adderall adderall caffeine caffeine caffeine k...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>166</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1393586192625528832</td>\n",
       "      <td>alprazolam</td>\n",
       "      <td>@justky1018 See if you can get your doctor to ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>129</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.024545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1561452418285547520</td>\n",
       "      <td>diazepam</td>\n",
       "      <td>@feytaline Reminds me of the time I had a roug...</td>\n",
       "      <td>positive</td>\n",
       "      <td>236</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id     therapy  \\\n",
       "0  1550591923047600131    cannabis   \n",
       "1  1496301299691839491    adderall   \n",
       "2  1460587790966657024    adderall   \n",
       "3  1393586192625528832  alprazolam   \n",
       "4  1561452418285547520    diazepam   \n",
       "\n",
       "                                                text     label  body_len  \\\n",
       "0  @chuckschumer YES. Please. Cannabis is legal i...   neutral       239   \n",
       "1  @youdoingtoomuch I’m a busy girl, adderall kee...  positive        67   \n",
       "2  adderall adderall caffeine caffeine caffeine k...   neutral       166   \n",
       "3  @justky1018 See if you can get your doctor to ...   neutral       129   \n",
       "4  @feytaline Reminds me of the time I had a roug...  positive       236   \n",
       "\n",
       "   punct%  sentiments  \n",
       "0     6.3   -0.075000  \n",
       "1     6.0    0.437500  \n",
       "2     3.6    0.150000  \n",
       "3     2.3    0.024545  \n",
       "4     4.7    0.087500  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data before vectorizing\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6c0bb",
   "metadata": {},
   "source": [
    "### Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9972467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>sentiments</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>7694</th>\n",
       "      <th>7695</th>\n",
       "      <th>7696</th>\n",
       "      <th>7697</th>\n",
       "      <th>7698</th>\n",
       "      <th>7699</th>\n",
       "      <th>7700</th>\n",
       "      <th>7701</th>\n",
       "      <th>7702</th>\n",
       "      <th>7703</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>5.3</td>\n",
       "      <td>-0.278846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-0.229167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7707 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%  sentiments    0    1    2    3    4    5    6  ...  7694  \\\n",
       "0       301     5.3   -0.278846  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1        47     2.1    0.375000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "2       125     1.6   -0.229167  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "3       112     1.8   -0.175000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "4       171     3.5   -0.041667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "   7695  7696  7697  7698  7699  7700  7701  7702  7703  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 7707 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a TfidfVectorizer object with the analyzer parameter set to the clean_text function\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "# Fitting the TfidfVectorizer on the 'text' column of the training set\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['text'])\n",
    "\n",
    "# Transforming the 'text' column of the training and testing sets into TF-IDF features\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['text'])\n",
    "\n",
    "# Concatenating the 'body_len' and 'punct%' and 'sentiments' columns with the TF-IDF features of the training set\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punct%', 'sentiments']].reset_index(drop=True), \n",
    "                          pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "\n",
    "# Concatenating the 'body_len' and 'punct%'  and 'sentiments'columns with the TF-IDF features of the testing set\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punct%', 'sentiments']].reset_index(drop=True), \n",
    "                         pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "# Displaying the head (first few rows) of the X_train_vect DataFrame\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d015785",
   "metadata": {},
   "source": [
    "### Final evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cae1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17f836e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SOFTWARE\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Average Precision: 0.8657654650645306\n",
      "Macro Average Recall: 0.5748310044095516\n",
      "Macro Average F1-score: 0.6346955869918399\n",
      "Support: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SOFTWARE\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing the RandomForestClassifier from the sklearn.ensemble module\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "# Creating a RandomForestClassifier object with specified parameters\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "# Measuring the time taken to fit (train) the RandomForestClassifier on the training data\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "# Measuring the time taken to make predictions using the trained RandomForestClassifier \n",
    "# on the testing data\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "# Computing precision, recall, fscore, and support values for the predicted results\n",
    "precision, recall, fscore, support = score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Printing the precision, recall, and F1-score\n",
    "print('Macro Average Precision:', precision)\n",
    "print('Macro Average Recall:', recall)\n",
    "print('Macro Average F1-score:', fscore)\n",
    "print('Support:', support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d396de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SOFTWARE\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Average Precision: 0.6618217772834013\n",
      "Macro Average Recall: 0.6022616824435296\n",
      "Macro Average F1-score: 0.6258148392294735\n",
      "Support: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SOFTWARE\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing the GradientBoostingClassifier from the sklearn.ensemble module\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "# Creating a GradientBoostingClassifier object with specified parameters\n",
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "# Measuring the time taken to fit (train) the GradientBoostingClassifier on the training data\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "# Measuring the time taken to make predictions using the trained GradientBoostingClassifier on \n",
    "# the testing data\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "# Computing precision, recall, fscore, and support values for the predicted results\n",
    "precision, recall, fscore, support = score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Printing the precision, recall, and F1-score\n",
    "print('Macro Average Precision:', precision)\n",
    "print('Macro Average Recall:', recall)\n",
    "print('Macro Average F1-score:', fscore)\n",
    "print('Support:', support)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
