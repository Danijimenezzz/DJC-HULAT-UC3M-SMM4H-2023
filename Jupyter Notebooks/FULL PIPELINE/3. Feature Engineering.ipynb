{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0578e0",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6afef",
   "metadata": {},
   "source": [
    "We will create new features to train the model, gaining more insights from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf62fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\danij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\danij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\danij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download the nlkt tools\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32cf40a",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb48b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>therapy</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1526565065549352974</td>\n",
       "      <td>adderall</td>\n",
       "      <td>@danno6_ @LunaManokit I was able to quit adder...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>danno6 lunamanokit able quit adderall without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1494046188257087493</td>\n",
       "      <td>adderall</td>\n",
       "      <td>@samfuchsie me when i do adderall</td>\n",
       "      <td>neutral</td>\n",
       "      <td>samfuchsie adderall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1563293301930807298</td>\n",
       "      <td>adderall</td>\n",
       "      <td>@caslernoel Well' you didn't miss much,you alr...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>caslernoel well didnt miss muchyou already kne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500878265543704585</td>\n",
       "      <td>tramadol</td>\n",
       "      <td>Dolor neurop√°tico, corrientazos musculares, tr...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dolor neurop√°tico corrientazos musculares tram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1577193665705160705</td>\n",
       "      <td>cbd</td>\n",
       "      <td>My Medicine \\n#MentalHealthMatters #THC #CBD #...</td>\n",
       "      <td>positive</td>\n",
       "      <td>medicine mentalhealthmatters thc cbd ptsd ment...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id   therapy  \\\n",
       "0  1526565065549352974  adderall   \n",
       "1  1494046188257087493  adderall   \n",
       "2  1563293301930807298  adderall   \n",
       "3  1500878265543704585  tramadol   \n",
       "4  1577193665705160705       cbd   \n",
       "\n",
       "                                                text     label  \\\n",
       "0  @danno6_ @LunaManokit I was able to quit adder...   neutral   \n",
       "1                  @samfuchsie me when i do adderall   neutral   \n",
       "2  @caslernoel Well' you didn't miss much,you alr...   neutral   \n",
       "3  Dolor neurop√°tico, corrientazos musculares, tr...   neutral   \n",
       "4  My Medicine \\n#MentalHealthMatters #THC #CBD #...  positive   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  danno6 lunamanokit able quit adderall without ...  \n",
       "1                                samfuchsie adderall  \n",
       "2  caslernoel well didnt miss muchyou already kne...  \n",
       "3  dolor neurop√°tico corrientazos musculares tram...  \n",
       "4  medicine mentalhealthmatters thc cbd ptsd ment...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\cleaned_text.csv\")\n",
    "\n",
    "test_data = pd.read_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\test_cleaned_text.csv\")\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12b9a7",
   "metadata": {},
   "source": [
    "### 3.1 Body length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31135792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the 'count' function to the 'text' column and storing the result in \n",
    "# a new 'body_len' column. Not counting whitespaces\n",
    "data['body_len'] = data['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "test_data['body_len'] = test_data['text'].apply(lambda x: len(x) - x.count(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95285e",
   "metadata": {},
   "source": [
    "### 3.2 Count punctuation signs (% of punctuation in the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ef6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the percentage of punctuation characters in a given text\n",
    "def count_punct(text):\n",
    "    # Counting the number of punctuation characters in the text\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    # Calculating the percentage of punctuation characters (excluding spaces) in the text\n",
    "    return round(count/(len(text) - text.count(\" \")), 3) * 100\n",
    "\n",
    "\n",
    "# Applying the 'count_punct' function to the 'body_text' column and storing the result in \n",
    "# a new 'punct%' column\n",
    "data['punct%'] = data['text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "test_data['punct%'] = test_data['text'].apply(lambda x: count_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc84d1",
   "metadata": {},
   "source": [
    "### 3.3 Word with associated sentiment weight function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e452f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment intensity\n",
    "def get_sentiment_intensity(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "# Apply the function to the text column\n",
    "data['sentiment_intensity'] = data['cleaned_text'].apply(get_sentiment_intensity)\n",
    "\n",
    "test_data['sentiment_intensity'] = test_data['cleaned_text'].apply(get_sentiment_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6caa802",
   "metadata": {},
   "source": [
    "### 3.4 Words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad77000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count\n",
    "data['word_count'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "test_data['word_count'] = test_data['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "\n",
    "# Calculate character count\n",
    "data['char_count'] = data['text'].apply(lambda x: len(str(x)))\n",
    "test_data['char_count'] = test_data['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "\n",
    "# Calculate average word length\n",
    "data['avg_word_length'] = data['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_data['avg_word_length'] = test_data['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "\n",
    "# Calculate punctuation count\n",
    "data['punctuation_count'] = data['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test_data['punctuation_count'] = test_data['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "\n",
    "# Calculate hashtag count\n",
    "data['hashtag_count'] = data['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "test_data['hashtag_count'] = test_data['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "\n",
    "# Stopword Count\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "data['stopword_count'] = data['text'].apply(lambda x: len([word for word in x if word in stopwords]))\n",
    "test_data['stopword_count'] = test_data['text'].apply(lambda x: len([word for word in x if word in stopwords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987db617",
   "metadata": {},
   "source": [
    "### 3.5 Count positive and negative sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7817f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of Positive Words\n",
    "positive_words = [word for word, score in sia.lexicon.items() if score > 0]\n",
    "data['sia_positive_word_count'] = data['cleaned_text'].apply(lambda x: len([word for word in x if word in positive_words]))\n",
    "test_data['sia_positive_word_count'] = test_data['cleaned_text'].apply(lambda x: len([word for word in x if word in positive_words]))\n",
    "\n",
    "\n",
    "# Count of Negative Words\n",
    "negative_words = [word for word, score in sia.lexicon.items() if score < 0]\n",
    "data['sia_negative_word_count'] = data['cleaned_text'].apply(lambda x: len([word for word in x if word in negative_words]))\n",
    "test_data['sia_negative_word_count'] = test_data['cleaned_text'].apply(lambda x: len([word for word in x if word in negative_words]))\n",
    "\n",
    "\n",
    "# Positive Word Rate\n",
    "data['sia_positive_word_rate'] = data['sia_positive_word_count'] / data['word_count']\n",
    "test_data['sia_positive_word_rate'] = test_data['sia_positive_word_count'] / test_data['word_count']\n",
    "\n",
    "\n",
    "# Negative Word Rate\n",
    "data['sia_negative_word_rate'] = data['sia_negative_word_count'] / data['word_count']\n",
    "test_data['sia_negative_word_rate'] = test_data['sia_negative_word_count'] / test_data['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437b09c",
   "metadata": {},
   "source": [
    "### 3.6 More sentiment score features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0ea370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sentiment Scores\n",
    "sentiment_scores = data['cleaned_text'].apply(lambda x: sia.polarity_scores(x))\n",
    "test_sentiment_scores = test_data['cleaned_text'].apply(lambda x: sia.polarity_scores(x))\n",
    "\n",
    "\n",
    "# Positive Sentiment Score\n",
    "data['positive_score'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "test_data['positive_score'] = test_sentiment_scores.apply(lambda x: x['pos'])\n",
    "\n",
    "\n",
    "# Negative Sentiment Score\n",
    "data['negative_score'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "test_data['negative_score'] = test_sentiment_scores.apply(lambda x: x['neg'])\n",
    "\n",
    "\n",
    "# Neutral Sentiment Score\n",
    "data['neutral_score'] = sentiment_scores.apply(lambda x: x['neu'])\n",
    "test_data['neutral_score'] = test_sentiment_scores.apply(lambda x: x['neu'])\n",
    "\n",
    "\n",
    "# Compound Sentiment Score\n",
    "data['compound_score'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "test_data['compound_score'] = test_sentiment_scores.apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a9573",
   "metadata": {},
   "source": [
    "### 3.7 Count of sentiment expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ece032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of Laughing Expressions\n",
    "laugh_expressions = ['haha', 'hehe', 'lol']\n",
    "data['laugh_count'] = data['text'].apply(lambda x: sum([x.lower().count(expr) for expr in laugh_expressions]))\n",
    "\n",
    "\n",
    "# Count of Sad Expressions\n",
    "sad_expressions = [':(', ':-(', ';(', ';-(']\n",
    "data['sad_count'] = data['text'].apply(lambda x: sum([x.count(expr) for expr in sad_expressions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226a82c",
   "metadata": {},
   "source": [
    "### 3.8 Vader sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85ac9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VADER sentiment intensity analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to calculate VADER sentiment scores\n",
    "def get_vader_scores(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores\n",
    "\n",
    "# Calculate VADER sentiment scores for each tweet\n",
    "vader_scores = data['cleaned_text'].apply(get_vader_scores)\n",
    "\n",
    "# Extract compound score for each tweet\n",
    "data['compound_Vscore'] = vader_scores.apply(lambda x: x['compound'])\n",
    "\n",
    "# Extract negative score for each tweet\n",
    "data['negative_Vscore'] = vader_scores.apply(lambda x: x['neg'])\n",
    "\n",
    "# Extract neutral score for each tweet\n",
    "data['neutral_Vscore'] = vader_scores.apply(lambda x: x['neu'])\n",
    "\n",
    "# Extract positive score for each tweet\n",
    "data['positive_Vscore'] = vader_scores.apply(lambda x: x['pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351266a5",
   "metadata": {},
   "source": [
    "### Showing new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3091db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>therapy</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>sia_positive_word_count</th>\n",
       "      <th>sia_negative_word_count</th>\n",
       "      <th>sia_positive_word_rate</th>\n",
       "      <th>sia_negative_word_rate</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>laugh_count</th>\n",
       "      <th>sad_count</th>\n",
       "      <th>compound_Vscore</th>\n",
       "      <th>negative_Vscore</th>\n",
       "      <th>neutral_Vscore</th>\n",
       "      <th>positive_Vscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1454224517895688192</td>\n",
       "      <td>adderall</td>\n",
       "      <td>wait until i get an adderall prescription.  im...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wait get adderall prescription imma time every...</td>\n",
       "      <td>61</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "      <td>74</td>\n",
       "      <td>4.692308</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426258820376842243</td>\n",
       "      <td>oxycodone</td>\n",
       "      <td>@Sassychickie @kelly_rdc Fentanyl, OxyContin a...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sassychickie kellyrdc fentanyl oxycontin oxyco...</td>\n",
       "      <td>89</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "      <td>101</td>\n",
       "      <td>6.846154</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1473007602170798082</td>\n",
       "      <td>cbd</td>\n",
       "      <td>a fun juggling act of mine is taking adderall ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fun juggling act mine taking adderall drinking...</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>22</td>\n",
       "      <td>121</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1561156143405502466</td>\n",
       "      <td>percocet</td>\n",
       "      <td>percocet roxycodone with some xanax that i had...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>percocet roxycodone xanax crushed dust elevate...</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1559923718578741248</td>\n",
       "      <td>adderall</td>\n",
       "      <td>first day of adderall and i feel üòµ‚Äçüí´üòµ‚Äçüí´üòµ‚Äçüí´üòµ‚Äçüí´</td>\n",
       "      <td>negative</td>\n",
       "      <td>first day adderall feel</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id    therapy  \\\n",
       "0  1454224517895688192   adderall   \n",
       "1  1426258820376842243  oxycodone   \n",
       "2  1473007602170798082        cbd   \n",
       "3  1561156143405502466   percocet   \n",
       "4  1559923718578741248   adderall   \n",
       "\n",
       "                                                text     label  \\\n",
       "0  wait until i get an adderall prescription.  im...   neutral   \n",
       "1  @Sassychickie @kelly_rdc Fentanyl, OxyContin a...  negative   \n",
       "2  a fun juggling act of mine is taking adderall ...   neutral   \n",
       "3  percocet roxycodone with some xanax that i had...   neutral   \n",
       "4      first day of adderall and i feel üòµ‚Äçüí´üòµ‚Äçüí´üòµ‚Äçüí´üòµ‚Äçüí´  negative   \n",
       "\n",
       "                                        cleaned_text  body_len  punct%  \\\n",
       "0  wait get adderall prescription imma time every...        61     1.6   \n",
       "1  sassychickie kellyrdc fentanyl oxycontin oxyco...        89    10.1   \n",
       "2  fun juggling act mine taking adderall drinking...       100     1.0   \n",
       "3  percocet roxycodone xanax crushed dust elevate...       105     0.0   \n",
       "4                           first day adderall feel         38     0.0   \n",
       "\n",
       "   sentiment_intensity  word_count  char_count  avg_word_length  \\\n",
       "0               0.0000          13          74         4.692308   \n",
       "1               0.0000          13         101         6.846154   \n",
       "2               0.6249          22         121         4.545455   \n",
       "3              -0.4215          25         128         4.160000   \n",
       "4               0.0000           8          45         4.750000   \n",
       "\n",
       "   punctuation_count  hashtag_count  stopword_count  sia_positive_word_count  \\\n",
       "0                  1              0              29                        2   \n",
       "1                  9              0              30                        3   \n",
       "2                  1              0              43                        3   \n",
       "3                  0              0              57                        2   \n",
       "4                  0              0              14                        3   \n",
       "\n",
       "   sia_negative_word_count  sia_positive_word_rate  sia_negative_word_rate  \\\n",
       "0                        0                0.153846                     0.0   \n",
       "1                        0                0.230769                     0.0   \n",
       "2                        0                0.136364                     0.0   \n",
       "3                        0                0.080000                     0.0   \n",
       "4                        0                0.375000                     0.0   \n",
       "\n",
       "   positive_score  negative_score  neutral_score  compound_score  laugh_count  \\\n",
       "0           0.000           0.000          1.000          0.0000            0   \n",
       "1           0.000           0.000          1.000          0.0000            0   \n",
       "2           0.331           0.097          0.571          0.6249            0   \n",
       "3           0.000           0.219          0.781         -0.4215            0   \n",
       "4           0.000           0.000          1.000          0.0000            0   \n",
       "\n",
       "   sad_count  compound_Vscore  negative_Vscore  neutral_Vscore  \\\n",
       "0          0           0.0000            0.000           1.000   \n",
       "1          0           0.0000            0.000           1.000   \n",
       "2          0           0.6249            0.097           0.571   \n",
       "3          0          -0.4215            0.219           0.781   \n",
       "4          0           0.0000            0.000           1.000   \n",
       "\n",
       "   positive_Vscore  \n",
       "0            0.000  \n",
       "1            0.000  \n",
       "2            0.331  \n",
       "3            0.000  \n",
       "4            0.000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Assuming 'column_to_drop' is the name of the column you want to drop\n",
    "# data = data.drop('sentiment_scores', axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd2202aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store this dataframe with all of the features\n",
    "data.to_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\all_features.csv\", index=False)\n",
    "test_data.to_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\test_all_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42663f17",
   "metadata": {},
   "source": [
    "### Check feature importance to select the best features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a93317f",
   "metadata": {},
   "source": [
    "We will use a RandomForestClassifier in order to evaluate all of the features created and keep the most relevant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59f62452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.11      0.02      0.03        62\n",
      "     neutral       0.70      0.96      0.81       412\n",
      "    positive       0.60      0.14      0.23       128\n",
      "\n",
      "    accuracy                           0.69       602\n",
      "   macro avg       0.47      0.37      0.36       602\n",
      "weighted avg       0.62      0.69      0.61       602\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.32644978e-02, 5.71024926e-02, 4.98817390e-02, 5.23324498e-02,\n",
       "       6.26885927e-02, 7.41319807e-02, 4.68043458e-02, 1.26514158e-02,\n",
       "       6.36520698e-02, 4.27255146e-02, 0.00000000e+00, 6.77157455e-02,\n",
       "       0.00000000e+00, 4.66330638e-02, 4.33230458e-02, 6.35204675e-02,\n",
       "       4.83512901e-02, 2.74105979e-03, 7.57537030e-05, 4.84537447e-02,\n",
       "       4.29952935e-02, 6.51348532e-02, 4.58205837e-02])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the feature columns and the target column\n",
    "feature_columns = ['body_len', 'punct%', 'sentiment_intensity', 'word_count', 'char_count', 'avg_word_length',  \n",
    "                   'punctuation_count', 'hashtag_count', 'stopword_count', 'sia_positive_word_count', \n",
    "                   'sia_negative_word_count', 'sia_positive_word_rate', 'sia_negative_word_rate', \n",
    "                   'positive_score', 'negative_score', 'neutral_score', 'compound_score', 'laugh_count', 'sad_count', \n",
    "                   'compound_Vscore', 'negative_Vscore', 'neutral_Vscore', 'positive_Vscore']\n",
    "\n",
    "\n",
    "target_column = 'label'\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[feature_columns], data[target_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sentiment labels on the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Extract the feature importance\n",
    "feature_importance = rf.feature_importances_\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ce6cabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>avg_word_length</td>\n",
       "      <td>0.074132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sia_positive_word_rate</td>\n",
       "      <td>0.067716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>neutral_Vscore</td>\n",
       "      <td>0.065135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopword_count</td>\n",
       "      <td>0.063652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>neutral_score</td>\n",
       "      <td>0.063520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>body_len</td>\n",
       "      <td>0.063264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>char_count</td>\n",
       "      <td>0.062689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>punct%</td>\n",
       "      <td>0.057102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word_count</td>\n",
       "      <td>0.052332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentiment_intensity</td>\n",
       "      <td>0.049882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>compound_Vscore</td>\n",
       "      <td>0.048454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>compound_score</td>\n",
       "      <td>0.048351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>punctuation_count</td>\n",
       "      <td>0.046804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>positive_score</td>\n",
       "      <td>0.046633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>positive_Vscore</td>\n",
       "      <td>0.045821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>negative_score</td>\n",
       "      <td>0.043323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>negative_Vscore</td>\n",
       "      <td>0.042995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sia_positive_word_count</td>\n",
       "      <td>0.042726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hashtag_count</td>\n",
       "      <td>0.012651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>laugh_count</td>\n",
       "      <td>0.002741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sad_count</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sia_negative_word_rate</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sia_negative_word_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Feature  Importance\n",
       "5           avg_word_length    0.074132\n",
       "11   sia_positive_word_rate    0.067716\n",
       "21           neutral_Vscore    0.065135\n",
       "8            stopword_count    0.063652\n",
       "15            neutral_score    0.063520\n",
       "0                  body_len    0.063264\n",
       "4                char_count    0.062689\n",
       "1                    punct%    0.057102\n",
       "3                word_count    0.052332\n",
       "2       sentiment_intensity    0.049882\n",
       "19          compound_Vscore    0.048454\n",
       "16           compound_score    0.048351\n",
       "6         punctuation_count    0.046804\n",
       "13           positive_score    0.046633\n",
       "22          positive_Vscore    0.045821\n",
       "14           negative_score    0.043323\n",
       "20          negative_Vscore    0.042995\n",
       "9   sia_positive_word_count    0.042726\n",
       "7             hashtag_count    0.012651\n",
       "17              laugh_count    0.002741\n",
       "18                sad_count    0.000076\n",
       "12   sia_negative_word_rate    0.000000\n",
       "10  sia_negative_word_count    0.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame with the feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_columns, 'Importance': feature_importance})\n",
    "\n",
    "# Sort the DataFrame by importance score in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03664e",
   "metadata": {},
   "source": [
    "### Different way of evaluating features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39314de0",
   "metadata": {},
   "source": [
    "To get a more stable estimate of feature importance, you can train the Random Forest model multiple times with different random states, and then average the feature importances over all iterations.\n",
    "\n",
    "This will give you a more stable estimate of feature importance that is less likely to be affected by the randomness of the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb87e18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>avg_word_length</td>\n",
       "      <td>0.075084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sia_positive_word_rate</td>\n",
       "      <td>0.066511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>neutral_score</td>\n",
       "      <td>0.063762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>neutral_Vscore</td>\n",
       "      <td>0.063691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>char_count</td>\n",
       "      <td>0.063267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopword_count</td>\n",
       "      <td>0.063107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>body_len</td>\n",
       "      <td>0.062992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>punct%</td>\n",
       "      <td>0.057890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word_count</td>\n",
       "      <td>0.052507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>compound_score</td>\n",
       "      <td>0.048799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentiment_intensity</td>\n",
       "      <td>0.048773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>compound_Vscore</td>\n",
       "      <td>0.048654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>positive_Vscore</td>\n",
       "      <td>0.047227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>positive_score</td>\n",
       "      <td>0.047127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>punctuation_count</td>\n",
       "      <td>0.045390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sia_positive_word_count</td>\n",
       "      <td>0.043353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>negative_Vscore</td>\n",
       "      <td>0.043178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>negative_score</td>\n",
       "      <td>0.043122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hashtag_count</td>\n",
       "      <td>0.012701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>laugh_count</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sad_count</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sia_negative_word_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sia_negative_word_rate</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Feature  Importance\n",
       "5           avg_word_length    0.075084\n",
       "11   sia_positive_word_rate    0.066511\n",
       "15            neutral_score    0.063762\n",
       "21           neutral_Vscore    0.063691\n",
       "4                char_count    0.063267\n",
       "8            stopword_count    0.063107\n",
       "0                  body_len    0.062992\n",
       "1                    punct%    0.057890\n",
       "3                word_count    0.052507\n",
       "16           compound_score    0.048799\n",
       "2       sentiment_intensity    0.048773\n",
       "19          compound_Vscore    0.048654\n",
       "22          positive_Vscore    0.047227\n",
       "13           positive_score    0.047127\n",
       "6         punctuation_count    0.045390\n",
       "9   sia_positive_word_count    0.043353\n",
       "20          negative_Vscore    0.043178\n",
       "14           negative_score    0.043122\n",
       "7             hashtag_count    0.012701\n",
       "17              laugh_count    0.002800\n",
       "18                sad_count    0.000066\n",
       "10  sia_negative_word_count    0.000000\n",
       "12   sia_negative_word_rate    0.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize an array to store the feature importances\n",
    "feature_importances = np.zeros(len(feature_columns))\n",
    "\n",
    "# Number of iterations\n",
    "n_iterations = 200\n",
    "\n",
    "# Train the model multiple times with different random states\n",
    "for i in range(n_iterations):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=i)\n",
    "    rf.fit(X_train, y_train)\n",
    "    feature_importances += rf.feature_importances_\n",
    "\n",
    "# Average the feature importances\n",
    "feature_importances /= n_iterations\n",
    "\n",
    "# Create a DataFrame with the feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance score in descending order\n",
    "feature_importance_df_2 = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "feature_importance_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e3db7",
   "metadata": {},
   "source": [
    "As for taking into account the correlation between features, one method you can use is permutation importance. Permutation importance works by randomly shuffling a single feature and measuring how much the performance of the model decreases. This gives an estimate of how important the feature is.\n",
    "\n",
    "This will give you an estimate of feature importance that takes into account the correlation between features. However, please note that permutation importance can be more computationally intensive than the default feature importance from Random Forest, especially for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f12fd6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sia_positive_word_rate</td>\n",
       "      <td>0.005814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopword_count</td>\n",
       "      <td>0.004153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sia_positive_word_count</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>punctuation_count</td>\n",
       "      <td>0.003322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>laugh_count</td>\n",
       "      <td>0.002824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hashtag_count</td>\n",
       "      <td>0.001329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>body_len</td>\n",
       "      <td>0.000664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sia_negative_word_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sia_negative_word_rate</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sad_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentiment_intensity</td>\n",
       "      <td>-0.000664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>punct%</td>\n",
       "      <td>-0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>neutral_score</td>\n",
       "      <td>-0.001661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>compound_Vscore</td>\n",
       "      <td>-0.001827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>negative_Vscore</td>\n",
       "      <td>-0.001993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>negative_score</td>\n",
       "      <td>-0.002159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>char_count</td>\n",
       "      <td>-0.002326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>positive_Vscore</td>\n",
       "      <td>-0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>compound_score</td>\n",
       "      <td>-0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word_count</td>\n",
       "      <td>-0.002658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>neutral_Vscore</td>\n",
       "      <td>-0.002824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>avg_word_length</td>\n",
       "      <td>-0.002990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>positive_score</td>\n",
       "      <td>-0.003821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Feature  Importance\n",
       "11   sia_positive_word_rate    0.005814\n",
       "8            stopword_count    0.004153\n",
       "9   sia_positive_word_count    0.003488\n",
       "6         punctuation_count    0.003322\n",
       "17              laugh_count    0.002824\n",
       "7             hashtag_count    0.001329\n",
       "0                  body_len    0.000664\n",
       "10  sia_negative_word_count    0.000000\n",
       "12   sia_negative_word_rate    0.000000\n",
       "18                sad_count    0.000000\n",
       "2       sentiment_intensity   -0.000664\n",
       "1                    punct%   -0.000997\n",
       "15            neutral_score   -0.001661\n",
       "19          compound_Vscore   -0.001827\n",
       "20          negative_Vscore   -0.001993\n",
       "14           negative_score   -0.002159\n",
       "4                char_count   -0.002326\n",
       "22          positive_Vscore   -0.002492\n",
       "16           compound_score   -0.002492\n",
       "3                word_count   -0.002658\n",
       "21           neutral_Vscore   -0.002824\n",
       "5           avg_word_length   -0.002990\n",
       "13           positive_score   -0.003821"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Train the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Compute permutation importance\n",
    "result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Create a DataFrame with the feature names and their importance scores\n",
    "permutation_importance_df = pd.DataFrame({'Feature': feature_columns, 'Importance': result.importances_mean})\n",
    "\n",
    "# Sort the DataFrame by importance score in descending order\n",
    "permutation_importance_df = permutation_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "permutation_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15840c1",
   "metadata": {},
   "source": [
    "### What features will we keep?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d36a9",
   "metadata": {},
   "source": [
    "Based on the results, it seems that the features `sia_negative_word_rate`, `sia_negative_word_count`, `laugh_count`, `sad_count` consistently have zero or near-zero importance across all iterations. This suggests that these features do not contribute much to the model's predictions and could potentially be dropped.\n",
    "\n",
    "On the other hand, features like `avg_word_length`, `sia_positive_word_rate`, `stopword_count`, `char_count`, `body_len`, `neutral_score`, `neutral_Vscore`, `punct%`, `word_count`, `sentiment_intensity`, `compound_score`, `compound_Vscore`, `positive_Vscore`, `positive_score`, `sia_positive_word_count`, `punctuation_count`, `negative_Vscore`, `negative_score` consistently have higher importance values, suggesting that they are important for the model's predictions.\n",
    "\n",
    "However, it's important to note that feature importance doesn't tell the whole story. Even features with low importance could potentially be useful when combined with other features. Also, correlation between features can affect the importance values. \n",
    "\n",
    "A good next step could be to perform a more systematic feature selection process, such as recursive feature elimination or forward selection, to find the optimal set of features. \n",
    "\n",
    "Also, remember that these importance values are specific to the model (Random Forest in this case) and the specific data you're working with. If you plan to use a different model or if your data changes, the feature importance could change as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36ceff",
   "metadata": {},
   "source": [
    "### New dataframe just with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2004d230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'therapy', 'text', 'label', 'cleaned_text', 'body_len',\n",
       "       'punct%', 'sentiment_intensity', 'word_count', 'char_count',\n",
       "       'avg_word_length', 'punctuation_count', 'hashtag_count',\n",
       "       'stopword_count', 'sia_positive_word_count', 'sia_negative_word_count',\n",
       "       'sia_positive_word_rate', 'sia_negative_word_rate', 'positive_score',\n",
       "       'negative_score', 'neutral_score', 'compound_score', 'laugh_count',\n",
       "       'sad_count', 'compound_Vscore', 'negative_Vscore', 'neutral_Vscore',\n",
       "       'positive_Vscore'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "840a9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of the columns you want to keep\n",
    "data_cleaned_features = data[\n",
    "    ['tweet_id', 'therapy', 'label', 'cleaned_text', \n",
    "     'avg_word_length', 'sia_positive_word_rate', 'sia_negative_word_rate', 'neutral_score', 'stopword_count', \n",
    "     'body_len', 'compound_score', 'punct%', 'positive_score', 'negative_score', 'neutral_score']]\n",
    "\n",
    "\n",
    "test_data_cleaned_features = test_data[\n",
    "    ['tweet_id', 'therapy', 'label', 'cleaned_text', \n",
    "     'avg_word_length', 'sia_positive_word_rate', 'sia_negative_word_rate', 'neutral_score', 'stopword_count', \n",
    "     'body_len', 'compound_score', 'punct%', 'positive_score', 'negative_score', 'neutral_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "043a0abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3009\n",
      "753\n"
     ]
    }
   ],
   "source": [
    "print(len(data_cleaned_features))\n",
    "print(len(test_data_cleaned_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a5a1ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>therapy</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sia_positive_word_rate</th>\n",
       "      <th>sia_negative_word_rate</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>body_len</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>punct%</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1526565065549352974</td>\n",
       "      <td>adderall</td>\n",
       "      <td>neutral</td>\n",
       "      <td>danno6 lunamanokit able quit adderall without ...</td>\n",
       "      <td>4.463415</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810</td>\n",
       "      <td>88</td>\n",
       "      <td>185</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1494046188257087493</td>\n",
       "      <td>adderall</td>\n",
       "      <td>neutral</td>\n",
       "      <td>samfuchsie adderall</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1563293301930807298</td>\n",
       "      <td>adderall</td>\n",
       "      <td>neutral</td>\n",
       "      <td>caslernoel well didnt miss muchyou already kne...</td>\n",
       "      <td>5.348837</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699</td>\n",
       "      <td>100</td>\n",
       "      <td>231</td>\n",
       "      <td>-0.6435</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500878265543704585</td>\n",
       "      <td>tramadol</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dolor neurop√°tico corrientazos musculares tram...</td>\n",
       "      <td>6.611111</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>112</td>\n",
       "      <td>239</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1577193665705160705</td>\n",
       "      <td>cbd</td>\n",
       "      <td>positive</td>\n",
       "      <td>medicine mentalhealthmatters thc cbd ptsd ment...</td>\n",
       "      <td>11.086957</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>80</td>\n",
       "      <td>259</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id   therapy     label  \\\n",
       "0  1526565065549352974  adderall   neutral   \n",
       "1  1494046188257087493  adderall   neutral   \n",
       "2  1563293301930807298  adderall   neutral   \n",
       "3  1500878265543704585  tramadol   neutral   \n",
       "4  1577193665705160705       cbd  positive   \n",
       "\n",
       "                                        cleaned_text  avg_word_length  \\\n",
       "0  danno6 lunamanokit able quit adderall without ...         4.463415   \n",
       "1                                samfuchsie adderall         4.666667   \n",
       "2  caslernoel well didnt miss muchyou already kne...         5.348837   \n",
       "3  dolor neurop√°tico corrientazos musculares tram...         6.611111   \n",
       "4  medicine mentalhealthmatters thc cbd ptsd ment...        11.086957   \n",
       "\n",
       "   sia_positive_word_rate  sia_negative_word_rate  neutral_score  \\\n",
       "0                0.195122                     0.0          0.810   \n",
       "1                0.333333                     0.0          1.000   \n",
       "2                0.348837                     0.0          0.699   \n",
       "3                0.388889                     0.0          1.000   \n",
       "4                0.478261                     0.0          1.000   \n",
       "\n",
       "   stopword_count  body_len  compound_score  punct%  positive_score  \\\n",
       "0              88       185          0.5719     4.3           0.190   \n",
       "1              13        28          0.0000     3.6           0.000   \n",
       "2             100       231         -0.6435     6.5           0.103   \n",
       "3             112       239          0.0000     7.1           0.000   \n",
       "4              80       259          0.0000     9.7           0.000   \n",
       "\n",
       "   negative_score  neutral_score  \n",
       "0           0.000          0.810  \n",
       "1           0.000          1.000  \n",
       "2           0.198          0.699  \n",
       "3           0.000          1.000  \n",
       "4           0.000          1.000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_cleaned_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27762937",
   "metadata": {},
   "source": [
    "### Write out the new dataframe with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a360480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it to our data files\n",
    "data_cleaned_features.to_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\data_cleaned_features.csv\", mode='w', index=False)\n",
    "\n",
    "test_data_cleaned_features.to_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\test_data_cleaned_features.csv\", mode='w', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
