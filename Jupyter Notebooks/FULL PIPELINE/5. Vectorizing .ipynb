{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69c1e8d",
   "metadata": {},
   "source": [
    "# Vectorizing - TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5125305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ab5db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>therapy</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sia_positive_word_rate</th>\n",
       "      <th>sia_negative_word_rate</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>body_len</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>punct%</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1526565065549352974</td>\n",
       "      <td>adderall</td>\n",
       "      <td>neutral</td>\n",
       "      <td>danno6 lunamanokit able quit adderall without ...</td>\n",
       "      <td>4.463415</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810</td>\n",
       "      <td>88</td>\n",
       "      <td>185</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1494046188257087493</td>\n",
       "      <td>adderall</td>\n",
       "      <td>neutral</td>\n",
       "      <td>samfuchsie adderall</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1563293301930807298</td>\n",
       "      <td>adderall</td>\n",
       "      <td>neutral</td>\n",
       "      <td>caslernoel well didnt miss muchyou already kne...</td>\n",
       "      <td>5.348837</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699</td>\n",
       "      <td>100</td>\n",
       "      <td>231</td>\n",
       "      <td>-0.6435</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500878265543704585</td>\n",
       "      <td>tramadol</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dolor neuropático corrientazos musculares tram...</td>\n",
       "      <td>6.611111</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>112</td>\n",
       "      <td>239</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1577193665705160705</td>\n",
       "      <td>cbd</td>\n",
       "      <td>positive</td>\n",
       "      <td>medicine mentalhealthmatters thc cbd ptsd ment...</td>\n",
       "      <td>11.086957</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>80</td>\n",
       "      <td>259</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id   therapy     label  \\\n",
       "0  1526565065549352974  adderall   neutral   \n",
       "1  1494046188257087493  adderall   neutral   \n",
       "2  1563293301930807298  adderall   neutral   \n",
       "3  1500878265543704585  tramadol   neutral   \n",
       "4  1577193665705160705       cbd  positive   \n",
       "\n",
       "                                        cleaned_text  avg_word_length  \\\n",
       "0  danno6 lunamanokit able quit adderall without ...         4.463415   \n",
       "1                                samfuchsie adderall         4.666667   \n",
       "2  caslernoel well didnt miss muchyou already kne...         5.348837   \n",
       "3  dolor neuropático corrientazos musculares tram...         6.611111   \n",
       "4  medicine mentalhealthmatters thc cbd ptsd ment...        11.086957   \n",
       "\n",
       "   sia_positive_word_rate  sia_negative_word_rate  neutral_score  \\\n",
       "0                0.195122                     0.0          0.810   \n",
       "1                0.333333                     0.0          1.000   \n",
       "2                0.348837                     0.0          0.699   \n",
       "3                0.388889                     0.0          1.000   \n",
       "4                0.478261                     0.0          1.000   \n",
       "\n",
       "   stopword_count  body_len  compound_score  punct%  positive_score  \\\n",
       "0              88       185          0.5719     4.3           0.190   \n",
       "1              13        28          0.0000     3.6           0.000   \n",
       "2             100       231         -0.6435     6.5           0.103   \n",
       "3             112       239          0.0000     7.1           0.000   \n",
       "4              80       259          0.0000     9.7           0.000   \n",
       "\n",
       "   negative_score  neutral_score.1  \n",
       "0           0.000            0.810  \n",
       "1           0.000            1.000  \n",
       "2           0.198            0.699  \n",
       "3           0.000            1.000  \n",
       "4           0.000            1.000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\data_cleaned_features.csv\")\n",
    "\n",
    "test_data = pd.read_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\test_data_cleaned_features.csv\")\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7698a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3009\n",
      "753\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f9124",
   "metadata": {},
   "source": [
    "### Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842564bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "\n",
    "# The 'label' column is used as the target variable (y)\n",
    "# The rest of the columns except 'tweet_id' & 'therapy' are used as the features (X)\n",
    "# The test_size parameter is set to 0.2, which means 20% of the data will be used for testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\n",
    "    ['tweet_id', 'therapy', 'cleaned_text', \n",
    "     'avg_word_length', 'sia_positive_word_rate', 'sia_negative_word_rate', 'neutral_score', 'stopword_count', \n",
    "     'body_len', 'compound_score', 'punct%', 'positive_score', 'negative_score', 'neutral_score']\n",
    "], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3126e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[['tweet_id', 'therapy', 'cleaned_text',\n",
    "               'avg_word_length', 'sia_positive_word_rate', 'sia_negative_word_rate',\n",
    "               'neutral_score', 'stopword_count', 'body_len', 'compound_score',\n",
    "               'punct%', 'positive_score', 'negative_score', 'neutral_score']]\n",
    "\n",
    "X_test = test_data[['tweet_id', 'therapy', 'cleaned_text',\n",
    "                    'avg_word_length', 'sia_positive_word_rate', 'sia_negative_word_rate',\n",
    "                    'neutral_score', 'stopword_count', 'body_len', 'compound_score',\n",
    "                    'punct%', 'positive_score', 'negative_score', 'neutral_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31dcbe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c444075",
   "metadata": {},
   "source": [
    "### Write out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdfe2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\train_features.csv', mode='w', index=False)\n",
    "# X_val.to_csv('C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\val_features.csv', mode='w', index=False)\n",
    "X_test.to_csv('C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\test_features.csv', mode='w', index=False)\n",
    "\n",
    "y_train.to_csv('C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\train_labels.csv', mode='w', index=False)\n",
    "# y_val.to_csv('C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\val_labels.csv', mode='w', index=False)\n",
    "y_test.to_csv('C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\test_labels.csv', mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae023cc",
   "metadata": {},
   "source": [
    "### TD-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32963453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sia_positive_word_rate</th>\n",
       "      <th>sia_negative_word_rate</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>body_len</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>punct%</th>\n",
       "      <th>...</th>\n",
       "      <th>11361</th>\n",
       "      <th>11362</th>\n",
       "      <th>11363</th>\n",
       "      <th>11364</th>\n",
       "      <th>11365</th>\n",
       "      <th>11366</th>\n",
       "      <th>11367</th>\n",
       "      <th>11368</th>\n",
       "      <th>11369</th>\n",
       "      <th>11370</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1454224517895688192</td>\n",
       "      <td>4.692308</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>29</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426258820376842243</td>\n",
       "      <td>6.846154</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>30</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1473007602170798082</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.571</td>\n",
       "      <td>43</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1561156143405502466</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.781</td>\n",
       "      <td>57</td>\n",
       "      <td>105</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1559923718578741248</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id  avg_word_length  sia_positive_word_rate  \\\n",
       "0  1454224517895688192         4.692308                0.153846   \n",
       "1  1426258820376842243         6.846154                0.230769   \n",
       "2  1473007602170798082         4.545455                0.136364   \n",
       "3  1561156143405502466         4.160000                0.080000   \n",
       "4  1559923718578741248         4.750000                0.375000   \n",
       "\n",
       "   sia_negative_word_rate  neutral_score  neutral_score  stopword_count  \\\n",
       "0                     0.0          1.000          1.000              29   \n",
       "1                     0.0          1.000          1.000              30   \n",
       "2                     0.0          0.571          0.571              43   \n",
       "3                     0.0          0.781          0.781              57   \n",
       "4                     0.0          1.000          1.000              14   \n",
       "\n",
       "   body_len  compound_score  punct%  ...  11361  11362  11363  11364  11365  \\\n",
       "0        61          0.0000     1.6  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "1        89          0.0000    10.1  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "2       100          0.6249     1.0  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "3       105         -0.4215     0.0  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "4        38          0.0000     0.0  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   11366  11367  11368  11369  11370  \n",
       "0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 11385 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a TfidfVectorizer object with the analyzer parameter set to the clean_text function\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# Fitting the TfidfVectorizer on the 'text' column of the training set\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['cleaned_text'])\n",
    "\n",
    "# Transforming the 'text' column of the training and testing sets into TF-IDF features\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['cleaned_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['cleaned_text'])\n",
    "\n",
    "# Concatenating the features columns with the TF-IDF features of the training set\n",
    "X_train_vect = pd.concat([X_train[\n",
    "    ['tweet_id', 'avg_word_length', 'sia_positive_word_rate', 'sia_negative_word_rate', 'neutral_score', \n",
    "     'stopword_count', 'body_len', 'compound_score', 'punct%', 'positive_score', 'negative_score', 'neutral_score']\n",
    "].reset_index(drop=True), pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "\n",
    "# Concatenating the features columns with the TF-IDF features of the testing set\n",
    "X_test_vect = pd.concat([X_test[\n",
    "    ['tweet_id', 'avg_word_length', 'sia_positive_word_rate', 'sia_negative_word_rate', 'neutral_score', \n",
    "     'stopword_count', 'body_len', 'compound_score', 'punct%', 'positive_score', 'negative_score', 'neutral_score']\n",
    "].reset_index(drop=True), pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "# Displaying the head (first few rows) of the X_train_vect DataFrame\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64be7a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sia_positive_word_rate</th>\n",
       "      <th>sia_negative_word_rate</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>body_len</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>punct%</th>\n",
       "      <th>...</th>\n",
       "      <th>11361</th>\n",
       "      <th>11362</th>\n",
       "      <th>11363</th>\n",
       "      <th>11364</th>\n",
       "      <th>11365</th>\n",
       "      <th>11366</th>\n",
       "      <th>11367</th>\n",
       "      <th>11368</th>\n",
       "      <th>11369</th>\n",
       "      <th>11370</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1526565065549352974</td>\n",
       "      <td>4.463415</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.810</td>\n",
       "      <td>88</td>\n",
       "      <td>185</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>4.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1494046188257087493</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1563293301930807298</td>\n",
       "      <td>5.348837</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>100</td>\n",
       "      <td>231</td>\n",
       "      <td>-0.6435</td>\n",
       "      <td>6.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500878265543704585</td>\n",
       "      <td>6.611111</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>112</td>\n",
       "      <td>239</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1577193665705160705</td>\n",
       "      <td>11.086957</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>80</td>\n",
       "      <td>259</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id  avg_word_length  sia_positive_word_rate  \\\n",
       "0  1526565065549352974         4.463415                0.195122   \n",
       "1  1494046188257087493         4.666667                0.333333   \n",
       "2  1563293301930807298         5.348837                0.348837   \n",
       "3  1500878265543704585         6.611111                0.388889   \n",
       "4  1577193665705160705        11.086957                0.478261   \n",
       "\n",
       "   sia_negative_word_rate  neutral_score  neutral_score  stopword_count  \\\n",
       "0                     0.0          0.810          0.810              88   \n",
       "1                     0.0          1.000          1.000              13   \n",
       "2                     0.0          0.699          0.699             100   \n",
       "3                     0.0          1.000          1.000             112   \n",
       "4                     0.0          1.000          1.000              80   \n",
       "\n",
       "   body_len  compound_score  punct%  ...  11361  11362  11363  11364  11365  \\\n",
       "0       185          0.5719     4.3  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "1        28          0.0000     3.6  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "2       231         -0.6435     6.5  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "3       239          0.0000     7.1  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "4       259          0.0000     9.7  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   11366  11367  11368  11369  11370  \n",
       "0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 11385 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the head (first few rows) of the X_train_vect DataFrame\n",
    "X_test_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198d1aa",
   "metadata": {},
   "source": [
    "### Save vectorized dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4585782",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect.to_csv('C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\X_train_vect.csv', mode='w', index=False)\n",
    "\n",
    "X_test_vect.to_csv('C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\X_test_vect.csv', mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c08867",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a CountVectorizer object with ngram_range=(2,2) to create bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the text column of the dataframe\n",
    "bigrams = vectorizer.fit_transform(data_cleaned_features['cleaned_text'])\n",
    "\n",
    "# Convert the result to a dataframe\n",
    "df_bigrams = pd.DataFrame(bigrams.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df_bigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2977f",
   "metadata": {},
   "source": [
    "###  Final features Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, reset the index of all dataframes to ensure they align correctly\n",
    "X_train_vect.reset_index(drop=True, inplace=True)\n",
    "df_bigrams.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Then, concatenate all the dataframes along axis=1 (i.e., columns)\n",
    "df_all_features = pd.concat([X_train_vect, df_bigrams], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed44a7",
   "metadata": {},
   "source": [
    "### Save final features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8362755",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\tdidf_ngrams_features.csv\", mode='w', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
