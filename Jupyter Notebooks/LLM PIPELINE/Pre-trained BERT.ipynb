{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d96999",
   "metadata": {},
   "source": [
    "# USING A PRE-TRAINED BERT MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c68b2",
   "metadata": {},
   "source": [
    "The task involves a multi-class classification problem for sentiment analysis in English tweets. Given the nature of the data and the task, a strong approach would be to use a pre-trained transformer model like BERT, which has shown great success in NLP tasks, including sentiment analysis.\n",
    "\n",
    "Below, I'll outline the main steps of the code and provide an example implementation. The code will include the following:\n",
    "\n",
    "1. Data Loading and Preprocessing: Read the CSV file and preprocess the text.\n",
    "2. Model Selection: Use a pre-trained BERT model for classification.\n",
    "3. Training: Train the model on the provided training data.\n",
    "4. Evaluation: Evaluate the model using the validation set and the micro-averaged F1-score.\n",
    "5. Prediction: Generate predictions on the test set.\n",
    "6. Submission File Creation: Create the submission file as per the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc266d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d2be7",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    inputs = tokenizer(df['text'].tolist(), padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "    label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "    labels = torch.tensor(df['label'].map(label_mapping).tolist())\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = load_data(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\train.csv\")\n",
    "valid_dataset = load_data(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\dev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04120b",
   "metadata": {},
   "source": [
    "### Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset\n",
    "label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "class_weights = torch.tensor([len(train_df)/train_df['label'].value_counts()[label] for label in label_mapping.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56162aa",
   "metadata": {},
   "source": [
    "### Model and Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3589dc4",
   "metadata": {},
   "source": [
    "We'll use the transformers library to load a pre-trained BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c04d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DistilBert model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "total_steps = len(train_dataset) * 3 # Number of epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27681efa",
   "metadata": {},
   "source": [
    "### Step 3: Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90244a1b",
   "metadata": {},
   "source": [
    "The training process involves tokenizing the text and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d43261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataset = train_df[['text', 'label']]\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Define Trainer and train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851dbd3b",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fe940",
   "metadata": {},
   "source": [
    "Evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e01239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize validation data\n",
    "val_dataset = val_df[['text', 'label']]\n",
    "val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(val_loader)\n",
    "\n",
    "# Calculate micro-averaged F1 score\n",
    "f1_micro = f1_score(val_df['label'], predictions, average='micro')\n",
    "print(\"Micro-averaged F1-score:\", f1_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a65d9",
   "metadata": {},
   "source": [
    "### Step 5: Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fd0b4",
   "metadata": {},
   "source": [
    "Predict the labels for the test set and create the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f2e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume test_df contains the test data\n",
    "test_dataset = test_df[['text']]\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Get test predictions\n",
    "test_predictions = trainer.predict(test_loader)\n",
    "\n",
    "# Decode labels\n",
    "test_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({'tweet_id': test_df['tweet_id'], 'label': test_labels})\n",
    "submission_df.to_csv('answer.txt', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
