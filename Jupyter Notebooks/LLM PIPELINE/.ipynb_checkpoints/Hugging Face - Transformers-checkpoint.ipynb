{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf4d5f4",
   "metadata": {},
   "source": [
    "# PIPELINE: USING Hugging Face - Transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc995a4",
   "metadata": {},
   "source": [
    "Step-by-step solution to create a functional code for the task of multi-class sentiment classification of tweets. I will use the Hugging Face transformers library, which provides access to a wide range of pre-trained language models, including BERT, RoBERTa, and more. For this example, I'll use the DistilBertForSequenceClassification model, which is a lightweight variant of BERT that is well-suited for sequence classification tasks like sentiment analysis.\n",
    "\n",
    "Here's an outline of the steps we'll follow:\n",
    "\n",
    "1. Install the necessary Python packages.\n",
    "2. Preprocess the data.\n",
    "3. Load the pre-trained model and tokenizer from Hugging Face.\n",
    "4. Fine-tune the model on the training data.\n",
    "5. Evaluate the model on the validation data.\n",
    "6. Generate predictions on the test data and save them in the required submission format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75996fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca445f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8376e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544726b",
   "metadata": {},
   "source": [
    "### Step 1: Install the necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae73b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b709e898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2394b6acb3cb4015a35fd26f329af106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SOFTWARE\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\danij\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02f4326ae2444e39c1c3b6623dee1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5e03bc44784ea4b21a0d2a91737dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b9ad2",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343fc744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "def load_data(file_path):\n",
    "    # Read the data from the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Tokenize the text, pad or truncate to a fixed length, and convert to integer IDs\n",
    "    inputs = tokenizer(df['text'].tolist(), padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "    \n",
    "    # Convert the labels to integers\n",
    "    label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "    labels = torch.tensor(df['label'].map(label_mapping).tolist())\n",
    "    \n",
    "    # Create a TensorDataset from the tokenized inputs and labels\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7021cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and validation data\n",
    "train_dataset = load_data(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\train.csv\")\n",
    "valid_dataset = load_data(\"C:\\\\Users\\\\danij\\\\Documents\\\\UC3M\\\\TFG\\\\DATA\\\\dev.csv\")\n",
    "\n",
    "# Concatenate the train and dev data\n",
    "# data = pd.concat([train_data, dev_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8169b0",
   "metadata": {},
   "source": [
    "### Step 3: Load the pre-trained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6bdc1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f2cebd4e8f4e19bd40a18ff20b884b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\SOFTWARE\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the DistilBert model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102fdb8",
   "metadata": {},
   "source": [
    "### Step 4: Fine-tune the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4459ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model on the training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bca82e",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the model on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8ed7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.53      0.57      0.55       148\n",
      "    negative       0.32      0.45      0.38        94\n",
      "     neutral       0.83      0.75      0.79       511\n",
      "\n",
      "    accuracy                           0.68       753\n",
      "   macro avg       0.56      0.59      0.57       753\n",
      "weighted avg       0.71      0.68      0.69       753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation data\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "predictions, true_labels = [], []\n",
    "model.eval()\n",
    "for batch in valid_loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    preds = torch.argmax(outputs.logits, dim=1)\n",
    "    predictions.extend(preds.tolist())\n",
    "    true_labels.extend(labels.tolist())\n",
    "print(classification_report(true_labels, predictions, target_names=['positive', 'negative', 'neutral']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b51126",
   "metadata": {},
   "source": [
    "### Step 6: Generate predictions on the test data and save them in the required submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a617059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the test data\n",
    "test_df = pd.read_csv(\"testing.csv\")\n",
    "test_inputs = tokenizer(test_df['text'].tolist(), padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d318707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test data\n",
    "test_predictions = []\n",
    "for batch in test_loader:\n",
    "    input_ids, attention_mask = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    preds = torch.argmax(outputs.logits, dim=1)\n",
    "    test_predictions.extend(preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ae3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions in the required submission format\n",
    "submission_df = pd.DataFrame({'tweet_id': test_df['tweet_id'], 'label': test_predictions})\n",
    "submission_df['label'] = submission_df['label'].replace({0: 'positive', 1: 'negative', 2: 'neutral'})\n",
    "submission_df.to_csv(\"answer.txt\", sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
